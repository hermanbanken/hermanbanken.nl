---
title: xDS and Go, or DIY service mesh
author: hbanken
type: post
date: 2022-05-19T10:00:00+02:00
url: /2022/05/19/k8s-xds/
categories:
  - grpc
  - envoy
  - xds
  - go
  - kubernetes

---

NB. this is the blog version of a talk I gave at Golang Rotterdam. 
The slides can be found at [Google Slides](https://docs.google.com/presentation/d/1M49ZGRg09KeRH76oT6gD2oMJN8fpSQu9BX8-0NqLuLo/edit?usp=sharing),
and the source repository for all samples (unless indicated otherwise) is at [github.com/hermanbanken/k8s-xds](https://github.com/hermanbanken/k8s-xds).

---------

This could be the story of how an important service (5k+ req/s) at Philips Hue went down for a short while.
We could explore how it might not be a good idea to configure a gRPC client to target a Kubernetes Service with `ClusterIP=None`.
How this means that if the target deployment restarts each pod in quick succession, and completes within 30 seconds, 
there might not be a single backend left that receives traffic. But that would not be constructive.

Lets look at how you could create a gRPC client in Go, that makes a request to `backend-service` every second:

```go
// Connect to server
c, err := grpc.DialContext(ctx, "backend-service")
if err != nil {
    log.Fatal(err.Error())
}
client := examplev1.NewExampleClient(c)

// Make continuous requests
for {
    req := &examplev1.ExampleRequest{}
    client.DoSomething(ctx, req)

    time.Sleep(1 * time.Second)
}
```

gRPC will use the DNS resolver by default (to make it explicit we could have used `dns:///backend-service` instead). For this case it matters a lot what `backend-service` is. Is it a simple Kubernetes ClusterIP Service, or a headless service? Or is it an internal/external load balanced service?

The DNS resolver has a control loop that resolves at most every 30 seconds, and has a back-off mechanism up to 2 minutes if the resolve fails:

```go
// https://github.com/grpc/grpc-go/blob/db79903af928ca8307700d83123a7702881c4e3c/internal/resolver/dns/dns_resolver.go#L84-L86
// To prevent excessive re-resolution, we enforce a rate limit on DNS resolution requests.
minDNSResRate = 30 * time.Second

// https://pkg.go.dev/google.golang.org/grpc@v1.46.2/backoff#DefaultConfig
backoff.DefaultExponential.MaxDelay = 120 * time.Second

// https://github.com/grpc/grpc-go/blob/db79903af928ca8307700d83123a7702881c4e3c/internal/resolver/dns/dns_resolver.go#L209-L245
for {
  state, err := d.lookup()
  if err != nil {
    // Report error to the underlying grpc.ClientConn.
    d.cc.ReportError(err)
  } else {
    err = d.cc.UpdateState(*state)
  }

  var timer *time.Timer
  if err == nil {
    // Success resolving, wait for the next ResolveNow. However, also wait 30 seconds at the very least
    // to prevent constantly re-resolving.
    backoffIndex = 1
    timer = newTimerDNSResRate(minDNSResRate)
    select {
    case <-d.ctx.Done():
      timer.Stop()
      return
    case <-d.rn:
    }
  } else {
    // Poll on an error found in DNS Resolver or an error received from ClientConn.
    timer = newTimer(backoff.DefaultExponential.Backoff(backoffIndex))
    backoffIndex++
  }
  select {
  case <-d.ctx.Done():
    timer.Stop()
    return
  case <-timer.C:
  }
}
```

If we simply use a Kubernetes ClusterIP Service for this `backend-service`, then `kube-proxy` will be used to route connections from `service-frontend` to `service-backend`, but each frontend will only open a single connection to a single backend, because the TCP/HTTP2 connection is kept alive.
We can fiddle with server options like `grpc.KeepaliveParams` to rebalance regularly, but in the end the distribution is never truly balanced.

```go
grpc.KeepaliveParams(keepalive.ServerParameters{
  MaxConnectionAge:      10 * time.Second,
  MaxConnectionAgeGrace: 30 * time.Second,
})
```

If we use a Kubernetes `ClusterIP=None` service, KubeDNS will return all pod ip addresses that match the service. So gRPC can create multiple connections and with `grpc.WithBalancerName(roundrobin.Name)` it will Round Robin all requests over available destinations.

-----

Try it today, and let me know if it helped you or if you have any feedback! Reach out to me at [Twitter](https://twitter.com/hermanbanken).
